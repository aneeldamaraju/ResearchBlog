<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Using BeRT for inpainting squares Part 2: BeRT for Rectangles | Aneel&#39;s Research Blog</title>
<meta name="keywords" content="">
<meta name="description" content="A brief update on previous results Since the last report, the inpainting task from the last report ended up working pretty well! One large change made for stabilization is that each individual training image is shown to the model multiple times in a row (Presentations per Epoch) to stabilize training. This seemed to work, but may promote memorizing, i.e. worse generalization! Here are the parameters used in the successful trial:">
<meta name="author" content="Me">
<link rel="canonical" href="https://aneeldamaraju.github.io/ResearchBlog/posts/12-16-2022-rectangleinpaint-weekly/">
<meta name="google-site-verification" content="XYZabc">
<meta name="yandex-verification" content="XYZabc">
<meta name="msvalidate.01" content="XYZabc">
<link crossorigin="anonymous" href="/ResearchBlog/assets/css/stylesheet.bc1149f4a72aa4858d3a9f71462f75e5884ffe8073ea9d6d5761d5663d651e20.css" integrity="sha256-vBFJ9KcqpIWNOp9xRi915YhP/oBz6p1tV2HVZj1lHiA=" rel="preload stylesheet" as="style">
<script defer crossorigin="anonymous" src="/ResearchBlog/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js" integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG&#43;9vmJ0cTS&#43;ovo0FeA="
    onload="hljs.initHighlightingOnLoad();"></script>
<link rel="icon" href="https://aneeldamaraju.github.io/ResearchBlog/%3Clink%20/%20abs%20url%3E">
<link rel="icon" type="image/png" sizes="16x16" href="https://aneeldamaraju.github.io/ResearchBlog/%3Clink%20/%20abs%20url%3E">
<link rel="icon" type="image/png" sizes="32x32" href="https://aneeldamaraju.github.io/ResearchBlog/%3Clink%20/%20abs%20url%3E">
<link rel="apple-touch-icon" href="https://aneeldamaraju.github.io/ResearchBlog/%3Clink%20/%20abs%20url%3E">
<link rel="mask-icon" href="https://aneeldamaraju.github.io/ResearchBlog/%3Clink%20/%20abs%20url%3E">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --hljs-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css" integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js" integrity="sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js" integrity="sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa" crossorigin="anonymous"
    onload="renderMathInElement(document.body, 
    {
              delimiters: [
                  {left: '$$', right: '$$', display: true},
                  {left: '\\[', right: '\\]', display: true},
                  {left: '$', right: '$', display: false},
                  {left: '\\(', right: '\\)', display: false}
              ]
          }
    );"></script>
 <meta property="og:title" content="Using BeRT for inpainting squares Part 2: BeRT for Rectangles" />
<meta property="og:description" content="A brief update on previous results Since the last report, the inpainting task from the last report ended up working pretty well! One large change made for stabilization is that each individual training image is shown to the model multiple times in a row (Presentations per Epoch) to stabilize training. This seemed to work, but may promote memorizing, i.e. worse generalization! Here are the parameters used in the successful trial:" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://aneeldamaraju.github.io/ResearchBlog/posts/12-16-2022-rectangleinpaint-weekly/" />
<meta property="og:image" content="https://aneeldamaraju.github.io/ResearchBlog/%3Cimage%20path/url%3E" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-12-16T00:00:00+00:00" />
<meta property="article:modified_time" content="2022-12-16T00:00:00+00:00" /><meta property="og:site_name" content="Aneel&#39;s Research Blog" />

<meta name="twitter:card" content="summary_large_image" />
<meta name="twitter:image" content="https://aneeldamaraju.github.io/ResearchBlog/%3Cimage%20path/url%3E" />
<meta name="twitter:title" content="Using BeRT for inpainting squares Part 2: BeRT for Rectangles"/>
<meta name="twitter:description" content="A brief update on previous results Since the last report, the inpainting task from the last report ended up working pretty well! One large change made for stabilization is that each individual training image is shown to the model multiple times in a row (Presentations per Epoch) to stabilize training. This seemed to work, but may promote memorizing, i.e. worse generalization! Here are the parameters used in the successful trial:"/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Posts",
      "item": "https://aneeldamaraju.github.io/ResearchBlog/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  3 ,
      "name": "Using BeRT for inpainting squares Part 2: BeRT for Rectangles",
      "item": "https://aneeldamaraju.github.io/ResearchBlog/posts/12-16-2022-rectangleinpaint-weekly/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Using BeRT for inpainting squares Part 2: BeRT for Rectangles",
  "name": "Using BeRT for inpainting squares Part 2: BeRT for Rectangles",
  "description": "A brief update on previous results Since the last report, the inpainting task from the last report ended up working pretty well! One large change made for stabilization is that each individual training image is shown to the model multiple times in a row (Presentations per Epoch) to stabilize training. This seemed to work, but may promote memorizing, i.e. worse generalization! Here are the parameters used in the successful trial:",
  "keywords": [
    
  ],
  "articleBody": "A brief update on previous results Since the last report, the inpainting task from the last report ended up working pretty well! One large change made for stabilization is that each individual training image is shown to the model multiple times in a row (Presentations per Epoch) to stabilize training. This seemed to work, but may promote memorizing, i.e. worse generalization! Here are the parameters used in the successful trial:\nTo showcase the results, I use two similar problems. First is the problem of masking out a single pixel and seeing if the transformer can solve the task of correctly inpainting the results.\nSingle pixel masking training loss essentially reached perfect accuracy\nA second slightly harder task involves keeping all else equal and increasing the number of masked pixels from a single pixel to 20 percent of the image. The transformer is able to succeed on this task as well.\nMany pixel masking results\nMoving from squares to rectangles Okay, so this step is not as simple as the title may make it sound. The big difference in this work compared to the previous experiments is that we massively increase the resolution of the input image to $256 X 256$. Since the number of transformer parameters scales ~quadratically with the number of input pixels, we have to apply a smart “downsampling” (or tokenization) technique so that the number of tokens is within a computationally feasible range (less than 1000). For natural images, a common tokenization technique is to use the quantizer from a pretrained vector-quantized GAN network.\nQuantization for rectangles In our case, we do not need a quantization scheme as complex as a VQGAN, so we try the more interpretable and easier to implement method of k-means clustering. Each cluster center will correspond to a token, and any patch can be mapped to the closest centroid, and assigned its corresponding token.\nSpecifics To create this set of cluster centers, we generate 100 random rectangles of resolution $256 X 256$ and first downsample them to $16 X 16$. We use area downsampling to capture how many of the pixels were black or white.\nGenerating a rectangle, and then downsampling with a factor of 16\nWe then take every overlapping $4X4$ patch of this $16X16$ and flatten each of these patches into a length $16$ vector. These vectors are then clustered into 20 clusters. The cluster centers are shown below.\nK-means clustering of patches of downsampled rectangles\nFirst pass: Inpainting on a Grid In order to ensure that the previously created model was robust enough to classify the increased number of tokens from the quantization scheme for the rectangular problem, we test the efficacy of that model on this new task. For each rectangle, the model is fed all overlapping patches (for example, in a $16x16$ image the model is fed $13X13$ tokens). Like in the previous task, 20 percent of these tokens are masked out and the model tries to guess what these tokens are. At inference time, these tokens are then converted to grayscale images by replacing them with their corresponding cluster centroid, and stitched together to create an output image. This works well! Results are shown below.\nSome test results of grid based rectangle inpainting\nSparse Token Prediction (Working Title) From this problem we take the jump to a much more difficult problem of sparse token prediction on the image. Where in the previous case we had a total of $169$ tokens each corresponding to the same $169$ locations across images, we now move to the smaller space of having a total of $24$ tokens in each image. In addition, each of these tokens can now correspond to any possible continuous coordinate in the image, and we can generate the correct corresponding tokens due to knowing the ground truth rectangle used to generate the image.\nThe training scheme for this task is very naive.\nGenerate a set of “training rectangles” (100) At training time: Pick 24 random points within each image (image here used loosely because technically we treat each rectangle and its corresponding subspace in two dimensions as an infinite resolution image), and get their corresponding images and tokens. Mask out 20 percent of these points and try to estimate the true token of the masked tokens from their coordinates. For the dataset of 100 images this model with minimal hyperparameter tuning is able to achieve decent training accuracy, as shown below:\nTraining loss for the training scheme listed above\nHowever for out of sample rectangles, this model does not seem to work well at all which implies that the model is memorizing the training rectangles.\nSome low quality preliminary results. The ground truth rectangle is represented in red.\nThis could also mean that the model has the capacity to solve this problem, and training just needs to be engineered in such a way that this memorization does not happen as often. For example, using a loss different from cross entropy or trying a different scheme for sampling points or simply generating a new rectangle at every training step.\nFuture Work The next steps for this problem include trying some of the smarter methods mentioned in the previous section. In addition, there are also some other directions to take this work. For example, one way to make this problem easier would be to remove all patches that contain corners from the image. However, in a $16x16$ image, this would drop out a large number of patches (for $4x4$ patches, this would remove around $4x16 = 64$ patches out of a total $169$ patches) that otherwise may contain important information about the edges of the rectangle.\n",
  "wordCount" : "938",
  "inLanguage": "en",
  "image":"https://aneeldamaraju.github.io/ResearchBlog/%3Cimage%20path/url%3E","datePublished": "2022-12-16T00:00:00Z",
  "dateModified": "2022-12-16T00:00:00Z",
  "author":{
    "@type": "Person",
    "name": "Me"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://aneeldamaraju.github.io/ResearchBlog/posts/12-16-2022-rectangleinpaint-weekly/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Aneel's Research Blog",
    "logo": {
      "@type": "ImageObject",
      "url": "https://aneeldamaraju.github.io/ResearchBlog/%3Clink%20/%20abs%20url%3E"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://aneeldamaraju.github.io/ResearchBlog" accesskey="h" title="🏠 (Alt + H)">
                <img src="https://aneeldamaraju.github.io/apple-touch-icon.png" alt="" aria-label="logo"
                    height="35">🏠</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://aneeldamaraju.github.io/ResearchBlog">Home</a>&nbsp;»&nbsp;<a href="https://aneeldamaraju.github.io/ResearchBlog/posts/">Posts</a></div>
    <h1 class="post-title">
      Using BeRT for inpainting squares Part 2: BeRT for Rectangles
    </h1>
    <div class="post-meta"><span title='2022-12-16 00:00:00 +0000 UTC'>December 16, 2022</span>&nbsp;·&nbsp;5 min&nbsp;·&nbsp;938 words&nbsp;·&nbsp;Me

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><nav id="TableOfContents">
  <ul>
    <li><a href="#a-brief-update-on-previous-results">A brief update on previous results</a></li>
    <li><a href="#moving-from-squares-to-rectangles">Moving from squares to rectangles</a>
      <ul>
        <li><a href="#quantization-for-rectangles">Quantization for rectangles</a></li>
        <li><a href="#first-pass-inpainting-on-a-grid">First pass: Inpainting on a Grid</a></li>
      </ul>
    </li>
    <li><a href="#sparse-token-prediction-working-title">Sparse Token Prediction (Working Title)</a></li>
    <li><a href="#future-work">Future Work</a></li>
  </ul>
</nav>
        </div>
    </details>
</div>

  <div class="post-content"><h2 id="a-brief-update-on-previous-results">A brief update on previous results<a hidden class="anchor" aria-hidden="true" href="#a-brief-update-on-previous-results">#</a></h2>
<p>Since the last report, the inpainting task from the last report ended up working pretty well! One large change made for stabilization is that each individual training image is shown to the model multiple times in a row (Presentations per Epoch) to stabilize training. This seemed to work, but may promote memorizing, i.e. worse generalization! Here are the parameters used in the successful trial:</p>

<p>To showcase the results, I use two similar problems. First is the problem of masking out a single pixel and seeing if the transformer can solve the task of correctly inpainting the results.</p>
<figure class="align-center ">
    <img loading="lazy" src="images/SinglePixelSquareMaskingTrainingLoss.png#center"
         alt="Single pixel masking training loss essentially reached perfect accuracy" width="800"/> <figcaption>
            <p>Single pixel masking training loss essentially reached perfect accuracy</p>
        </figcaption>
</figure>

<p>A second slightly harder task involves keeping all else equal and increasing the number of masked pixels from a single pixel to 20 percent of the image. The transformer is able to succeed on this task as well.</p>
<figure class="align-center ">
    <img loading="lazy" src="images/20PercentSquareMasking.png#center"
         alt="Many pixel masking results" width="800"/> <figcaption>
            <p>Many pixel masking results</p>
        </figcaption>
</figure>

<h2 id="moving-from-squares-to-rectangles">Moving from squares to rectangles<a hidden class="anchor" aria-hidden="true" href="#moving-from-squares-to-rectangles">#</a></h2>
<p>Okay, so this step is not as simple as the title may make it sound. The big difference in this work compared to the previous experiments is that we massively increase the resolution of the input image to $256 X 256$. Since the number of transformer parameters scales ~quadratically with the number of input pixels, we have to apply a smart &ldquo;downsampling&rdquo; (or tokenization) technique so that the number of tokens is within a computationally feasible range (less than 1000). For natural images, a common tokenization technique is to use the quantizer from a pretrained vector-quantized GAN network.</p>
<h3 id="quantization-for-rectangles">Quantization for rectangles<a hidden class="anchor" aria-hidden="true" href="#quantization-for-rectangles">#</a></h3>
<p>In our case, we do not need a quantization scheme as complex as a VQGAN, so we try the more interpretable and easier to implement method of k-means clustering. Each cluster center will correspond to a token, and any patch can be mapped to the closest centroid, and assigned its corresponding token.</p>
<h4 id="specifics">Specifics<a hidden class="anchor" aria-hidden="true" href="#specifics">#</a></h4>
<p>To create this set of cluster centers, we generate 100 random rectangles of resolution $256 X 256$ and first downsample them to $16 X 16$. We use area downsampling to capture how many of the pixels were black or white.</p>
<figure class="align-center ">
    <img loading="lazy" src="images/DownsampledRectangle.png#center"
         alt="Generating a rectangle, and then downsampling with a factor of 16" width="800"/> <figcaption>
            <p>Generating a rectangle, and then downsampling with a factor of 16</p>
        </figcaption>
</figure>

<p>We then take every overlapping $4X4$ patch of this $16X16$ and flatten each of these patches into a length $16$ vector. These vectors are then clustered into 20 clusters. The cluster centers are shown below.</p>
<figure class="align-center ">
    <img loading="lazy" src="images/RectangleClusterCenters.png#center"
         alt="K-means clustering of patches of downsampled rectangles" width="800"/> <figcaption>
            <p>K-means clustering of patches of downsampled rectangles</p>
        </figcaption>
</figure>

<h3 id="first-pass-inpainting-on-a-grid">First pass: Inpainting on a Grid<a hidden class="anchor" aria-hidden="true" href="#first-pass-inpainting-on-a-grid">#</a></h3>
<p>In order to ensure that the previously created model was robust enough to classify the increased number of tokens from the quantization scheme for the rectangular problem, we test the efficacy of that model on this new task. For each rectangle, the model is fed all overlapping patches (for example, in a $16x16$ image the model is fed $13X13$ tokens). Like in the previous task, 20 percent of these tokens are masked out and the model tries to guess what these tokens are. At inference time, these tokens are then converted to grayscale images by replacing them with their corresponding cluster centroid, and stitched together to create an output image. This works well! Results are shown below.</p>
<p><figure class="align-center ">
    <img loading="lazy" src="images/RectangleGridInpainting1.png#center" width="800"/> 
</figure>

<figure class="align-center ">
    <img loading="lazy" src="images/RectangleGridInpainting2.png#center"
         alt="Some test results of grid based rectangle inpainting" width="800"/> <figcaption>
            <p>Some test results of grid based rectangle inpainting</p>
        </figcaption>
</figure>
</p>
<h2 id="sparse-token-prediction-working-title">Sparse Token Prediction (Working Title)<a hidden class="anchor" aria-hidden="true" href="#sparse-token-prediction-working-title">#</a></h2>
<p>From this problem we take the jump to a much more difficult problem of sparse token prediction on the image. Where in the previous case we had a total of $169$ tokens each corresponding to the same $169$ locations across images, we now move to the smaller space of having a total of $24$ tokens in each image. In addition, each of these tokens can now correspond to any possible continuous coordinate in the image, and we can generate the correct corresponding tokens due to knowing the ground truth rectangle used to generate the image.</p>
<p>The training scheme for this task is very naive.</p>
<ol>
<li>Generate a set of &ldquo;training rectangles&rdquo; (100)
At training time:</li>
<li>Pick 24 random points within each image (image here used loosely because technically we treat each rectangle and its corresponding subspace in two dimensions as an infinite resolution image), and get their corresponding images and tokens.</li>
<li>Mask out 20 percent of these points and try to estimate the true token of the masked tokens from their coordinates.</li>
</ol>
<p>For the dataset of 100 images this model with minimal hyperparameter tuning is able to achieve decent training accuracy, as shown below:</p>
<figure class="align-center ">
    <img loading="lazy" src="images/RectangleContinuousTrainingLoss.png#center"
         alt="Training loss for the training scheme listed above" width="800"/> <figcaption>
            <p>Training loss for the training scheme listed above</p>
        </figcaption>
</figure>

<p>However for out of sample rectangles, this model does not seem to work well at all which implies that the model is memorizing the training rectangles.</p>
<figure class="align-center ">
    <img loading="lazy" src="images/RectangleTestResults.png#center"
         alt="Some low quality preliminary results. The ground truth rectangle is represented in red." width="800"/> <figcaption>
            <p>Some low quality preliminary results. The ground truth rectangle is represented in red.</p>
        </figcaption>
</figure>

<p>This could also mean that the model has the capacity to solve this problem, and training just needs to be engineered in such a way that this memorization does not happen as often. For example, using a loss different from cross entropy or trying a different scheme for sampling points or simply generating a new rectangle at every training step.</p>
<h2 id="future-work">Future Work<a hidden class="anchor" aria-hidden="true" href="#future-work">#</a></h2>
<p>The next steps for this problem include trying some of the smarter methods mentioned in the previous section. In addition, there are also some other directions to take this work. For example, one way to make this problem easier would be to remove all patches that contain corners from the image. However, in a $16x16$ image, this would drop out a large number of patches (for $4x4$ patches, this would remove around $4x16 = 64$ patches out of a total $169$ patches) that otherwise may contain important information about the edges of the rectangle.</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>
<nav class="paginav">
  <a class="next" href="https://aneeldamaraju.github.io/ResearchBlog/posts/10-25-2022-maskgit-weekly/">
    <span class="title">Next »</span>
    <br>
    <span>Using BeRT for inpainting squares</span>
  </a>
</nav>

  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2022 <a href="https://aneeldamaraju.github.io/ResearchBlog">Aneel&#39;s Research Blog</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
