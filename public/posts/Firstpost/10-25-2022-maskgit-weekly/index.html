<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Using BeRT for inpainting squares | My New Hugo Site</title>
<meta name="keywords" content="">
<meta name="description" content="A motivating toy problem Suppose I give you a prior in the form of a set of images of black and white squares on an axis-aligned grid.
Our prior: squares on a grid
Next, I give you the following images, and ask you to replace the gray (&ldquo;masked&rdquo;) squares with either black or white squares in order to fit your prior.
Our inputs: masked squares
Easy, right? Humans are able to leverage &ldquo;long-range&rdquo; (across image) information in order to determine that each image contains a single square, and the masked squares should be replaced with the correct colors that will make this square.">
<meta name="author" content="Me">
<link rel="canonical" href="https://aneeldamaraju.github.io/ResearchBlog/posts/firstpost/10-25-2022-maskgit-weekly/">
<meta name="google-site-verification" content="XYZabc">
<meta name="yandex-verification" content="XYZabc">
<meta name="msvalidate.01" content="XYZabc">
<link crossorigin="anonymous" href="/ResearchBlog/assets/css/stylesheet.bc1149f4a72aa4858d3a9f71462f75e5884ffe8073ea9d6d5761d5663d651e20.css" integrity="sha256-vBFJ9KcqpIWNOp9xRi915YhP/oBz6p1tV2HVZj1lHiA=" rel="preload stylesheet" as="style">
<script defer crossorigin="anonymous" src="/ResearchBlog/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js" integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG&#43;9vmJ0cTS&#43;ovo0FeA="
    onload="hljs.initHighlightingOnLoad();"></script>
<link rel="icon" href="https://aneeldamaraju.github.io/ResearchBlog/%3Clink%20/%20abs%20url%3E">
<link rel="icon" type="image/png" sizes="16x16" href="https://aneeldamaraju.github.io/ResearchBlog/%3Clink%20/%20abs%20url%3E">
<link rel="icon" type="image/png" sizes="32x32" href="https://aneeldamaraju.github.io/ResearchBlog/%3Clink%20/%20abs%20url%3E">
<link rel="apple-touch-icon" href="https://aneeldamaraju.github.io/ResearchBlog/%3Clink%20/%20abs%20url%3E">
<link rel="mask-icon" href="https://aneeldamaraju.github.io/ResearchBlog/%3Clink%20/%20abs%20url%3E">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --hljs-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css" integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js" integrity="sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js" integrity="sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa" crossorigin="anonymous"
    onload="renderMathInElement(document.body, 
    {
              delimiters: [
                  {left: '$$', right: '$$', display: true},
                  {left: '\\[', right: '\\]', display: true},
                  {left: '$', right: '$', display: false},
                  {left: '\\(', right: '\\)', display: false}
              ]
          }
    );"></script>
 <meta property="og:title" content="Using BeRT for inpainting squares" />
<meta property="og:description" content="A motivating toy problem Suppose I give you a prior in the form of a set of images of black and white squares on an axis-aligned grid.
Our prior: squares on a grid
Next, I give you the following images, and ask you to replace the gray (&ldquo;masked&rdquo;) squares with either black or white squares in order to fit your prior.
Our inputs: masked squares
Easy, right? Humans are able to leverage &ldquo;long-range&rdquo; (across image) information in order to determine that each image contains a single square, and the masked squares should be replaced with the correct colors that will make this square." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://aneeldamaraju.github.io/ResearchBlog/posts/firstpost/10-25-2022-maskgit-weekly/" />
<meta property="og:image" content="https://aneeldamaraju.github.io/ResearchBlog/%3Cimage%20path/url%3E" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-10-20T00:00:00+00:00" />
<meta property="article:modified_time" content="2022-10-20T00:00:00+00:00" /><meta property="og:site_name" content="ExampleSite" />

<meta name="twitter:card" content="summary_large_image" />
<meta name="twitter:image" content="https://aneeldamaraju.github.io/ResearchBlog/%3Cimage%20path/url%3E" />
<meta name="twitter:title" content="Using BeRT for inpainting squares"/>
<meta name="twitter:description" content="A motivating toy problem Suppose I give you a prior in the form of a set of images of black and white squares on an axis-aligned grid.
Our prior: squares on a grid
Next, I give you the following images, and ask you to replace the gray (&ldquo;masked&rdquo;) squares with either black or white squares in order to fit your prior.
Our inputs: masked squares
Easy, right? Humans are able to leverage &ldquo;long-range&rdquo; (across image) information in order to determine that each image contains a single square, and the masked squares should be replaced with the correct colors that will make this square."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Posts",
      "item": "https://aneeldamaraju.github.io/ResearchBlog/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  3 ,
      "name": "Using BeRT for inpainting squares",
      "item": "https://aneeldamaraju.github.io/ResearchBlog/posts/firstpost/10-25-2022-maskgit-weekly/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Using BeRT for inpainting squares",
  "name": "Using BeRT for inpainting squares",
  "description": "A motivating toy problem Suppose I give you a prior in the form of a set of images of black and white squares on an axis-aligned grid.\nOur prior: squares on a grid\nNext, I give you the following images, and ask you to replace the gray (\u0026ldquo;masked\u0026rdquo;) squares with either black or white squares in order to fit your prior.\nOur inputs: masked squares\nEasy, right? Humans are able to leverage \u0026ldquo;long-range\u0026rdquo; (across image) information in order to determine that each image contains a single square, and the masked squares should be replaced with the correct colors that will make this square.",
  "keywords": [
    
  ],
  "articleBody": "A motivating toy problem Suppose I give you a prior in the form of a set of images of black and white squares on an axis-aligned grid.\nOur prior: squares on a grid\nNext, I give you the following images, and ask you to replace the gray (“masked”) squares with either black or white squares in order to fit your prior.\nOur inputs: masked squares\nEasy, right? Humans are able to leverage “long-range” (across image) information in order to determine that each image contains a single square, and the masked squares should be replaced with the correct colors that will make this square.\nOur goal: Train a network to recover the squares\nSo why do we care about this simple example? Popular deep learning models in computer vision are built off of the back of Convolutional neural networks (CNNs), where neurons in the network each have a limited receptive field size. If the size of the receptive field is too small, the network will not have the ability to incorporate long-range information into it’s inference.\nSo instead of a CNN, we can instead use a bidirectional transformer (BERT) to solve our toy problem. BERT explicitly computes these long range interactions between squares in the image in the form of attention.\nA note on quantization BERT was initially developed as a natural language processing (NLP) model, so the authours had to find a way to convert the input words into numbers that can be interpreted by a neural network. BERT uses a common NLP approach, mapping each word to a a scalar, e.g. it’s index in a dictionary. Drawing analog to a black and white image, it makes sense to have a mapping dictionary consisting of two entries: black -\u003e 0, white -\u003e 1. For our example, we can represent the masked patches as a third dictionary entry [mask] -\u003e 2.\nThis idea of mapping elements to scalars can be extended to more complex images, and even to patches of images (check out vector quantized gans)! In particular, representing an image as quantized patch can be helpful in reducing the number of transformer parameters by reducing the number of attention weights calculated.\nThe key parts of the BERT model As commonly implemented, BERT is very simple (or at least more simple than the name would imply). BERT takes in quantized inputs and learns a sentence completion task by randomly masking out words in a sentence and learning to put the correct word back in place of the mask. We can draw analog to this task by training a network to inpaint images with arbitary masked patches.\nFor an image with some masked patches, BERT predicts the true value of the masked patch through a couple of key steps.\nEmbed the information of the patch (quantized color and positional information). Update the patch embedding with the attention-based weightings with all other patches. Predict the true value of the patch, usually through the use of a simple multi-layer perceptron (MLP) neural network. Model specifics Input data As shown in the motivating examples, each image in the training set is a 8x8 black and white image. Each image contains a square with a side length ranging from 2 to 6 units, located in a random location in the image. All possible permutations of this input results in 126 input images. Each image is then flattened into a 64 length vector, with each element chosen using the binary quantization scheme.\nMasking Before talking about embedding it is worth remembering that rather than two embeddings corresponding to black and white, we will need one more corresponding to a mask element for our image inpainting task. The masks are computed using the MaskGIT masking scheme, where a random fraction $\\gamma(r)$ of patches are masked out. Specifically: $$ \\gamma(r) = \\cos(\\frac{\\pi}{2} r) $$\nDuring training $r$ is drawn like $r \\sim Unif(0,1)$, while the assumption is that at inference time you will input a masked image into the model, so you will not provide $r$.\nEmbedding As a recap, each input to the model is a length 64 vector with each element being a key from the dictionary {0:black, 1:white, 2:[mask]}. Now we must embed this vector in an arbitary higher dimensional latent space that will correspond to the learned codebook. In my case I use a codebook dimension of $d_{model} = 768$ as used in MaskGIT.\nAlong side the codebook encoding, we also need to include a positional encoding to each encoded key. Treating the input image as a cartesian x-y grid ranging from (0,0) to (7,7), the sinsoidal positional encoding for each index is computed as:\n$$ pe(x,y,\\delta) = \\begin{cases} pe(x,\\delta) \u0026 \\text{if $\\delta \u003c d_{model}/2$} \\\\ pe(y,\\delta) \u0026 \\text{if $\\delta \\geq d_{model}/2$} \\end{cases} $$ where $\\delta$ is the dimension in the latent space, and $pe(x,\\delta)$ is given by the following. $$ pe(x,\\delta) = \\begin{cases} \\sin\\frac{x}{10000^{\\delta/d_{model}}} \u0026 \\text{if $\\delta$ is even} \\\\ \\cos\\frac{x}{10000^{\\delta/d_{model}}} \u0026 \\text{if $\\delta$ is odd} \\end{cases} $$ However this is not the only way implement positional encodings, so do not be surprised if a different method works better!\nTransformer architecture Encoder block At a high level, the easiest way to explain the encoder architecture is to see the code\n1 2 3 4 5 6 7 8 9 10 def forward(self, x): #Use built-in multihead attention, where x is the key, query and value attn, _ = self.MultiHeadAttention(x, x, x, need_weights=False) attn = self.dropout(attn) x = x.add(attn) x = self.LayerNorm1(x) mlp = self.MLP(x) x = x.add(mlp) x = self.LayerNorm2(x) return x where the MLP is simply:\n1 2 3 4 5 6 self.MLP = nn.Sequential(*[ nn.Linear(dim, hidden_dim), nn.GELU(), nn.Linear(hidden_dim, dim), nn.Dropout(p=0.1) ]) Other specifics of the model are taken from MaskGIT, including hidden_dim = 3072, dropout = 10% num_attention_heads = 8.\nPrediction block Token prediction in MaskGIT is done by using a two-layer MLP, that takes in the $64 X d_{model}$ dimensional inputs and returns outputs of the same dimension. These inputs are then mapped to a coresponding codebook key through cosine similarity with the learned codebook vectors. An alternative to this approach is to just have an MLP that predicts one hot encodings of each code, but this technique is not used in BERT.\nThe full transformer The full transformer simply consists of 6 encoder blocks, followed by a token predicton of the encoded inputs.\nModel training At every epoch each image is randomly masked following the masking scheme presented before. The images as well as their masked versions are passed into the model, with the goal of predicting the original image from the masked one.\nHowever, it does not seem to work! This is what the training loss looks like\nTraining loss fails to converge\n",
  "wordCount" : "1114",
  "inLanguage": "en",
  "image":"https://aneeldamaraju.github.io/ResearchBlog/%3Cimage%20path/url%3E","datePublished": "2022-10-20T00:00:00Z",
  "dateModified": "2022-10-20T00:00:00Z",
  "author":{
    "@type": "Person",
    "name": "Me"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://aneeldamaraju.github.io/ResearchBlog/posts/firstpost/10-25-2022-maskgit-weekly/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "My New Hugo Site",
    "logo": {
      "@type": "ImageObject",
      "url": "https://aneeldamaraju.github.io/ResearchBlog/%3Clink%20/%20abs%20url%3E"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://aneeldamaraju.github.io/ResearchBlog" accesskey="h" title="🏠 (Alt + H)">
                <img src="https://aneeldamaraju.github.io/apple-touch-icon.png" alt="" aria-label="logo"
                    height="35">🏠</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://aneeldamaraju.github.io/ResearchBlog">Home</a>&nbsp;»&nbsp;<a href="https://aneeldamaraju.github.io/ResearchBlog/posts/">Posts</a></div>
    <h1 class="post-title">
      Using BeRT for inpainting squares
    </h1>
    <div class="post-meta"><span title='2022-10-20 00:00:00 +0000 UTC'>October 20, 2022</span>&nbsp;·&nbsp;6 min&nbsp;·&nbsp;1114 words&nbsp;·&nbsp;Me

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><nav id="TableOfContents">
  <ul>
    <li><a href="#a-motivating-toy-problem">A motivating toy problem</a>
      <ul>
        <li><a href="#so-why-do-we-care-about-this-simple-example">So why do we care about this simple example?</a></li>
      </ul>
    </li>
  </ul>

  <ul>
    <li><a href="#input-data">Input data</a></li>
    <li><a href="#masking">Masking</a></li>
    <li><a href="#embedding">Embedding</a></li>
    <li><a href="#transformer-architecture">Transformer architecture</a>
      <ul>
        <li><a href="#encoder-block">Encoder block</a></li>
        <li><a href="#prediction-block">Prediction block</a></li>
        <li><a href="#the-full-transformer">The full transformer</a></li>
      </ul>
    </li>
    <li><a href="#model-training">Model training</a></li>
  </ul>
</nav>
        </div>
    </details>
</div>

  <div class="post-content"><h2 id="a-motivating-toy-problem">A motivating toy problem<a hidden class="anchor" aria-hidden="true" href="#a-motivating-toy-problem">#</a></h2>
<p>Suppose I give you a prior in the form of a set of images of black and white squares on an axis-aligned grid.</p>
<figure class="align-center ">
    <img loading="lazy" src="PriorSample.png#center"
         alt="Our prior: squares on a grid" width="400"/> <figcaption>
            <p>Our prior: squares on a grid</p>
        </figcaption>
</figure>

<p>Next, I give you the following images, and ask you to replace the gray (&ldquo;masked&rdquo;) squares with either black or white squares in order to fit your prior.</p>
<figure class="align-center ">
    <img loading="lazy" src="../MaskedSample2.png#center"
         alt="Our inputs: masked squares" width="400"/> <figcaption>
            <p>Our inputs: masked squares</p>
        </figcaption>
</figure>

<p>Easy, right? Humans are able to leverage &ldquo;long-range&rdquo; (across image) information in order to determine that each image contains a single square, and the masked squares should be replaced with the correct colors that will make this square.</p>
<figure class="align-center ">
    <img loading="lazy" src="images/MasktoPrior.png#center"
         alt="Our goal: Train a network to recover the squares" width="800"/> <figcaption>
            <p>Our goal: Train a network to recover the squares</p>
        </figcaption>
</figure>

<h3 id="so-why-do-we-care-about-this-simple-example">So why do we care about this simple example?<a hidden class="anchor" aria-hidden="true" href="#so-why-do-we-care-about-this-simple-example">#</a></h3>
<p>Popular deep learning models in computer vision are built off of the back of Convolutional neural networks (CNNs), where neurons in the network each have a limited receptive field size. If the size of the receptive field is too small, the network will not have the ability to incorporate long-range information into it&rsquo;s inference.</p>
<p>So instead of a CNN, we can instead use a bidirectional transformer (BERT) to solve our toy problem. BERT explicitly computes these long range interactions between squares in the image in the form of attention.</p>
<h1 id="a-note-on-quantization">A note on quantization<a hidden class="anchor" aria-hidden="true" href="#a-note-on-quantization">#</a></h1>
<p>BERT was initially developed as a natural language processing (NLP) model, so the authours had to find a way to convert the input words into numbers that can be interpreted by a neural network. BERT uses a common NLP approach, mapping each word to a a scalar, e.g. it&rsquo;s index in a dictionary. Drawing analog to a black and white image, it makes sense to have a mapping dictionary consisting of two entries: <code>black -&gt; 0, white -&gt; 1</code>. For our example, we can represent the masked patches as a third dictionary entry <code>[mask] -&gt; 2</code>.</p>
<p>This idea of mapping elements to scalars can be extended to more complex images, and even to patches of images (check out vector quantized gans)! In particular, representing an image as quantized patch can be helpful in reducing the number of transformer parameters by reducing the number of attention weights calculated.</p>
<h1 id="the-key-parts-of-the-bert-model">The key parts of the BERT model<a hidden class="anchor" aria-hidden="true" href="#the-key-parts-of-the-bert-model">#</a></h1>
<p>As commonly implemented, BERT is very simple (or at least more simple than the name would imply). BERT takes in quantized inputs and learns a sentence completion task by randomly masking out words in a sentence and learning to put the correct word back in place of the mask. We can draw analog to this task by training a network to inpaint images with arbitary masked patches.</p>
<p>For an image with some masked patches, BERT predicts the true value of the masked patch through a couple of key steps.</p>
<ol>
<li><strong>Embed</strong> the information of the patch (quantized color and positional information).</li>
<li><strong>Update</strong> the patch embedding with the <em>attention-based weightings</em> with all other patches.</li>
<li><strong>Predict</strong> the true value of the patch, usually through the use of a simple multi-layer perceptron (MLP) neural network.</li>
</ol>
<h1 id="model-specifics">Model specifics<a hidden class="anchor" aria-hidden="true" href="#model-specifics">#</a></h1>
<h2 id="input-data">Input data<a hidden class="anchor" aria-hidden="true" href="#input-data">#</a></h2>
<p>As shown in the motivating examples, each image in the training set is a 8x8 black and white image.  Each image contains a square with a <strong>side length ranging from 2 to 6 units</strong>, located in a random location in the image. All possible permutations of this input results in <strong>126 input images</strong>. Each image is then flattened into a 64 length vector, with each element chosen using the binary quantization scheme.</p>
<h2 id="masking">Masking<a hidden class="anchor" aria-hidden="true" href="#masking">#</a></h2>
<p>Before talking about embedding it is worth remembering that rather than two embeddings corresponding to black and white, we will need one more corresponding to a mask element for our image inpainting task. The masks are computed using the MaskGIT masking scheme, where a random fraction $\gamma(r)$ of patches are masked out. Specifically:
$$
\gamma(r) = \cos(\frac{\pi}{2} r)
$$</p>
<p>During training $r$ is drawn like $r \sim Unif(0,1)$, while the assumption is that at inference time you will input a masked image into the model, so you will not provide $r$.</p>
<h2 id="embedding">Embedding<a hidden class="anchor" aria-hidden="true" href="#embedding">#</a></h2>
<p>As a recap, each input to the model is a length 64 vector with each element being a key from the dictionary <code>{0:black, 1:white, 2:[mask]}</code>. Now we must embed this vector in an arbitary higher dimensional latent space that will correspond to the learned codebook. In my case I use a <strong>codebook dimension of $d_{model} = 768$</strong> as used in MaskGIT.</p>
<p>Along side the codebook encoding, we also need to include a positional encoding to each encoded key. Treating the input image as a cartesian x-y grid ranging from (0,0) to (7,7), the sinsoidal positional encoding for each index is computed as:</p>
<p>$$
pe(x,y,\delta) = \begin{cases}
pe(x,\delta) &amp; \text{if $\delta &lt; d_{model}/2$} \\
pe(y,\delta) &amp; \text{if $\delta \geq d_{model}/2$}
\end{cases}
$$
where $\delta$ is the dimension in the latent space, and $pe(x,\delta)$ is given by the following.
$$
pe(x,\delta) = \begin{cases}
\sin\frac{x}{10000^{\delta/d_{model}}} &amp; \text{if $\delta$ is even} \\
\cos\frac{x}{10000^{\delta/d_{model}}} &amp; \text{if $\delta$ is odd}
\end{cases}
$$
However this is not the only way implement positional encodings, so do not be surprised if a different method works better!</p>
<h2 id="transformer-architecture">Transformer architecture<a hidden class="anchor" aria-hidden="true" href="#transformer-architecture">#</a></h2>
<h3 id="encoder-block">Encoder block<a hidden class="anchor" aria-hidden="true" href="#encoder-block">#</a></h3>
<p>At a high level, the easiest way to explain the encoder architecture is to see the code</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="c1">#Use built-in multihead attention, where x is the key, query and value</span>
</span></span><span class="line"><span class="cl">    <span class="n">attn</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">MultiHeadAttention</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">need_weights</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">attn</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">attn</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">attn</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">LayerNorm1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">mlp</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">MLP</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">mlp</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">LayerNorm2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">x</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>where the MLP is simply:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="bp">self</span><span class="o">.</span><span class="n">MLP</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="o">*</span><span class="p">[</span>
</span></span><span class="line"><span class="cl">            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">            <span class="n">nn</span><span class="o">.</span><span class="n">GELU</span><span class="p">(),</span>
</span></span><span class="line"><span class="cl">            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="n">dim</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">            <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="p">])</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>Other specifics of the model are taken from MaskGIT, including <code>hidden_dim</code> = 3072, <code>dropout</code> = 10% <code>num_attention_heads</code> = 8.</p>
<h3 id="prediction-block">Prediction block<a hidden class="anchor" aria-hidden="true" href="#prediction-block">#</a></h3>
<p>Token prediction in MaskGIT is done by using a two-layer MLP, that takes in the $64 X d_{model}$ dimensional inputs and returns outputs of the same dimension. These inputs are then mapped to a coresponding codebook key through cosine similarity with the learned codebook vectors.  An alternative to this approach is to just have an MLP that predicts one hot encodings of each code, but this technique is not used in BERT.</p>
<h3 id="the-full-transformer">The full transformer<a hidden class="anchor" aria-hidden="true" href="#the-full-transformer">#</a></h3>
<p>The full transformer simply consists of <strong>6 encoder blocks</strong>, followed by a token predicton of the encoded inputs.</p>
<h2 id="model-training">Model training<a hidden class="anchor" aria-hidden="true" href="#model-training">#</a></h2>
<p>At every epoch each image is randomly masked following the masking scheme presented before. The images as well as their masked versions are passed into the model, with the goal of predicting the original image from the masked one.</p>
<p>However, it does not seem to work! This is what the training loss looks like</p>
<figure class="align-center ">
    <img loading="lazy" src="/posts/Firstpost/TrainingLoss.png#center"
         alt="Training loss fails to converge" width="400"/> <figcaption>
            <p>Training loss fails to converge</p>
        </figcaption>
</figure>



  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>
<nav class="paginav">
  <a class="prev" href="https://aneeldamaraju.github.io/ResearchBlog/posts/10-25-2022-maskgit-weekly/10-25-2022-maskgit-weekly/">
    <span class="title">« Prev</span>
    <br>
    <span>Using BeRT for inpainting squares</span>
  </a>
</nav>

  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2022 <a href="https://aneeldamaraju.github.io/ResearchBlog">My New Hugo Site</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
